{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teHzLo4owK3j"
      },
      "source": [
        "## LMQG Model Development (Gemini Device 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Model</b>\n",
        "<br>T5 Small / BART Base / T5 Base / T5 Large / BART Large\n",
        "<br>Gold QA (i.e., the model using the human-labeled gold\n",
        "annotations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>QAG approaches</b>\n",
        "<br>End2Endv / MultiTask / Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Dataset</b>\n",
        "<br>SQuADShifts (Amazon / Wiki / NYT / Reddit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Experiment</b>\n",
        "<br>All / Amazon / Wiki / NYT / Reddit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6m9o9YV0j7O"
      },
      "outputs": [],
      "source": [
        "%pip install lmqg\n",
        "%pip install peft\n",
        "%pip install easydict\n",
        "%pip install termcolor\n",
        "%pip install openpyxl\n",
        "%pip install ipywidgets\n",
        "!python -m spacy download en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XrYqvzLjZwev"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from pprint import pprint\n",
        "# from termcolor import coloredg\n",
        "from lmqg import TransformersQG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "2\n",
            "NVIDIA RTX A6000\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace\n"
          ]
        }
      ],
      "source": [
        "cd workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminseok0809\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flan T5 Small Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_small_squad_qg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lora Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_small_squad_qg_lora_r8_al16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_small_squad_qg_lora_r16_al16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_small_squad_qg_lora_r32_al16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_small_squad_qg_lora_r64_al16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_small_squad_qg_lora_r128_al16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_small_squad_qg_lora_r8_al32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_small_squad_qg_lora_r16_al32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_small_squad_qg_lora_r32_al32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_small_squad_qg_lora_r64_al32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_small_squad_qg_lora_r128_al32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flan T5 Large Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lora Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r1_al32_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r1_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r2_al32_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r2_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r4_al32_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r4_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r8_al32_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r8_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r16_al32_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r16_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r32_al32_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r32_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r64_al32_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r64_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r128_al32_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_large_squad_qg_lora_r128_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flan T5 XL Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lora Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r1_al32_training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r1_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r2_al32_training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r2_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminseok0809\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.10 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/wandb/run-20230907_041800-nycuje4h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mflan_t5_xl_squad_qg_lora_r4_al32\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/minseok0809/lmqg_qg_squad\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/minseok0809/lmqg_qg_squad/runs/nycuje4h\u001b[0m\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:24<00:00, 12.28s/it]\n",
            "\n",
            "\n",
            "trainable model parameters: 2359296\n",
            "all model parameters: 2852005888\n",
            "percentage of trainable model parameters: 0.08% \n",
            "\n",
            "\n",
            "\n",
            "Epoch 1: 100%|██████████████████████████| 18792/18792 [5:14:18<00:00,  1.00s/it]\n",
            "Loss :   3.8399\n",
            "\n",
            "\n",
            "Epoch 2: 100%|██████████████████████████| 18792/18792 [5:14:32<00:00,  1.00s/it]\n",
            "Loss :   0.605\n",
            "\n",
            "\n",
            "Epoch 3: 100%|██████████████████████████| 18792/18792 [5:14:40<00:00,  1.00s/it]\n",
            "Loss :   0.5794\n",
            "\n",
            "\n",
            "Epoch 4: 100%|██████████████████████████| 18792/18792 [5:14:40<00:00,  1.00s/it]\n",
            "Loss :   0.5689\n",
            "\n",
            "\n",
            "Epoch 5: 100%|██████████████████████████| 18792/18792 [5:14:57<00:00,  1.01s/it]\n",
            "Loss :   0.5621\n",
            "\n",
            "\n",
            "Epoch 6: 100%|██████████████████████████| 18792/18792 [5:15:20<00:00,  1.01s/it]\n",
            "Loss :   0.5567\n",
            "\n",
            "\n",
            "Epoch 7: 100%|██████████████████████████| 18792/18792 [5:15:21<00:00,  1.01s/it]\n",
            "Loss :   0.5526\n",
            "\n",
            "\n",
            "Epoch 8: 100%|██████████████████████████| 18792/18792 [5:15:23<00:00,  1.01s/it]\n",
            "Loss :   0.5484\n",
            "\n",
            "\n",
            "Epoch 9: 100%|██████████████████████████| 18792/18792 [5:15:35<00:00,  1.01s/it]\n",
            "Loss :   0.5442\n",
            "\n",
            "\n",
            "Epoch 10: 100%|█████████████████████████| 18792/18792 [5:16:09<00:00,  1.01s/it]\n",
            "Loss :   0.5407\n",
            "\n",
            "\n",
            "Epoch 11: 100%|█████████████████████████| 18792/18792 [5:16:28<00:00,  1.01s/it]\n",
            "Loss :   0.5367\n",
            "\n",
            "\n",
            "Epoch 12: 100%|█████████████████████████| 18792/18792 [5:16:23<00:00,  1.01s/it]\n",
            "Loss :   0.5337\n",
            "\n",
            "\n",
            "Epoch 13: 100%|█████████████████████████| 18792/18792 [5:16:11<00:00,  1.01s/it]\n",
            "Loss :   0.5306\n",
            "\n",
            "\n",
            "Epoch 14: 100%|█████████████████████████| 18792/18792 [5:16:40<00:00,  1.01s/it]\n",
            "Loss :   0.5274\n",
            "\n",
            "\n",
            "Epoch 15: 100%|█████████████████████████| 18792/18792 [5:17:01<00:00,  1.01s/it]\n",
            "Loss :   0.5246\n",
            "\n",
            "\n",
            "Epoch 16: 100%|█████████████████████████| 18792/18792 [5:16:53<00:00,  1.01s/it]\n",
            "Loss :   0.5215\n",
            "\n",
            "\n",
            "Epoch 17: 100%|█████████████████████████| 18792/18792 [5:17:01<00:00,  1.01s/it]\n",
            "Loss :   0.5189\n",
            "\n",
            "\n",
            "Epoch 18: 100%|█████████████████████████| 18792/18792 [5:17:12<00:00,  1.01s/it]\n",
            "Loss :   0.5163\n",
            "\n",
            "\n",
            "Epoch 19: 100%|█████████████████████████| 18792/18792 [5:17:51<00:00,  1.01s/it]\n",
            "Loss :   0.5142\n",
            "\n",
            "\n",
            "Epoch 20: 100%|█████████████████████████| 18792/18792 [5:17:39<00:00,  1.01s/it]\n",
            "Loss :   0.5118\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss 0.51181\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mflan_t5_xl_squad_qg_lora_r4_al32\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/minseok0809/lmqg_qg_squad/runs/nycuje4h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230907_041800-nycuje4h/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r4_al32_training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminseok0809\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.10 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/wandb/run-20230912_004753-ogzmfs0c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mflan_t5_xl_squad_qg_lora_r4_al32\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/minseok0809/lmqg_qg_squad\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/minseok0809/lmqg_qg_squad/runs/ogzmfs0c\u001b[0m\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.48s/it]\n",
            "100%|███████████████████████████████████| 10570/10570 [00:04<00:00, 2287.73it/s]\n",
            "\n",
            "\n",
            "Evaluation Time:  3:01:37\n",
            "Valid Bleu 1:  0.2211     Valid Bleu 2: 0.1382\n",
            "Valid Bleu 3:  0.0955     Valid Bleu 4:  0.0694\n",
            "Test Bleu 1:  0.2252     Test Bleu 2:  0.138\n",
            "Test Bleu 3:  0.093     Test Bleu 4:  0.0659\n",
            "\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.50s/it]\n",
            "\n",
            "\n",
            "Evaluation Time:  2:59:40\n",
            "Valid Bleu 1:  0.2749     Valid Bleu 2: 0.1811\n",
            "Valid Bleu 3:  0.1299     Valid Bleu 4:  0.0972\n",
            "Test Bleu 1:  0.2791     Test Bleu 2:  0.1811\n",
            "Test Bleu 3:  0.1272     Test Bleu 4:  0.0929\n",
            "\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.52s/it]\n",
            "\n",
            "\n",
            "Evaluation Time:  2:54:26\n",
            "Valid Bleu 1:  0.3662     Valid Bleu 2: 0.2537\n",
            "Valid Bleu 3:  0.1901     Valid Bleu 4:  0.1476\n",
            "Test Bleu 1:  0.3772     Test Bleu 2:  0.259\n",
            "Test Bleu 3:  0.1906     Test Bleu 4:  0.1449\n",
            "\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.52s/it]\n",
            "\n",
            "\n",
            "Evaluation Time:  2:44:32\n",
            "Valid Bleu 1:  0.462     Valid Bleu 2: 0.3313\n",
            "Valid Bleu 3:  0.2557     Valid Bleu 4:  0.2035\n",
            "Test Bleu 1:  0.4704     Test Bleu 2:  0.335\n",
            "Test Bleu 3:  0.2543     Test Bleu 4:  0.1983\n",
            "\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.50s/it]\n",
            "\n",
            "\n",
            "Evaluation Time:  2:38:30\n",
            "Valid Bleu 1:  0.5225     Valid Bleu 2: 0.3811\n",
            "Valid Bleu 3:  0.2979     Valid Bleu 4:  0.2397\n",
            "Test Bleu 1:  0.536     Test Bleu 2:  0.3882\n",
            "Test Bleu 3:  0.2995     Test Bleu 4:  0.2367\n",
            "\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.48s/it]\n",
            "\n",
            "\n",
            "Evaluation Time:  2:35:17\n",
            "Valid Bleu 1:  0.5469     Valid Bleu 2: 0.4024\n",
            "Valid Bleu 3:  0.3167     Valid Bleu 4:  0.2562\n",
            "Test Bleu 1:  0.5575     Test Bleu 2:  0.4069\n",
            "Test Bleu 3:  0.3158     Test Bleu 4:  0.251\n",
            "\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.16s/it]\n",
            "\n",
            "\n",
            "Evaluation Time:  2:33:10\n",
            "Valid Bleu 1:  0.5615     Valid Bleu 2: 0.4149\n",
            "Valid Bleu 3:  0.3277     Valid Bleu 4:  0.2659\n",
            "Test Bleu 1:  0.574     Test Bleu 2:  0.4205\n",
            "Test Bleu 3:  0.3273     Test Bleu 4:  0.2606\n",
            "\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.46s/it]\n",
            "\n",
            "\n",
            "Evaluation Time:  2:31:20\n",
            "Valid Bleu 1:  0.5722     Valid Bleu 2: 0.4235\n",
            "Valid Bleu 3:  0.335     Valid Bleu 4:  0.2723\n",
            "Test Bleu 1:  0.5854     Test Bleu 2:  0.4307\n",
            "Test Bleu 3:  0.3364     Test Bleu 4:  0.2688\n",
            "\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.13s/it]\n",
            "\n",
            "\n",
            "Evaluation Time:  2:31:35\n",
            "Valid Bleu 1:  0.5742     Valid Bleu 2: 0.4257\n",
            "Valid Bleu 3:  0.3372     Valid Bleu 4:  0.2745\n",
            "Test Bleu 1:  0.5885     Test Bleu 2:  0.4324\n",
            "Test Bleu 3:  0.3373     Test Bleu 4:  0.2692\n",
            "\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.49s/it]\n",
            "\n",
            "\n",
            "Evaluation Time:  2:31:36\n",
            "Valid Bleu 1:  0.5731     Valid Bleu 2: 0.4251\n",
            "Valid Bleu 3:  0.3366     Valid Bleu 4:  0.2734\n",
            "Test Bleu 1:  0.5877     Test Bleu 2:  0.4329\n",
            "Test Bleu 3:  0.3381     Test Bleu 4:  0.2702\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_bleu_1 ▁▂▄▆▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_bleu_2 ▁▂▄▆▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_bleu_3 ▁▂▄▆▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_bleu_4 ▁▂▄▆▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid_bleu_1 ▁▂▄▆▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid_bleu_2 ▁▂▄▆▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid_bleu_3 ▁▂▄▆▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid_bleu_4 ▁▂▄▆▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_bleu_1 0.58771\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_bleu_2 0.43288\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_bleu_3 0.33808\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_bleu_4 0.27017\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid_bleu_1 0.57312\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid_bleu_2 0.42507\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid_bleu_3 0.33655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid_bleu_4 0.27345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mflan_t5_xl_squad_qg_lora_r4_al32\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/minseok0809/lmqg_qg_squad/runs/ogzmfs0c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230912_004753-ogzmfs0c/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r4_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r8_al32_training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r8_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r16_al32_training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r16_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r32_al32_training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r32_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r64_al32_training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r64_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r128_al32_training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xl_squad_qg_lora_r128_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flan T5 XXL Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xxl_squad_qg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lora Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xxl_squad_qg_lora_r1_al32_training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xxl_squad_qg_lora_r1_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xxl_squad_qg_lora_r8_al32_training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lm-question-generation.lmqg.grid_searcher_flan_t5_xxl_squad_qg_lora_r8_al32_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erRMsEx-zJbX"
      },
      "source": [
        "## T5 Small Model Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lmqg import GridSearcher\n",
        "\n",
        "trainer = GridSearcher(\n",
        "    checkpoint_dir='tmp_ckpt_t5_small_0819',\n",
        "    dataset_path='lmqg/qg_squad',\n",
        "    model='t5-small',\n",
        "    epoch=3,\n",
        "    epoch_partial=1,\n",
        "    batch=64,\n",
        "    n_max_config=5,\n",
        "    gradient_accumulation_steps=[2], \n",
        "    lr=[1e-04],\n",
        "    label_smoothing=[0, 0.15]\n",
        ")\n",
        "trainer.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Grid Search\n",
        "<br>gradient_accumulation_steps=[2, 4], \n",
        "<br>lr=[1e-04, 5e-04, 1e-03],\n",
        "<br>label_smoothing=[0, 0.15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of model: \n",
        "<br>gradient_accumulation_steps * lr * label_smoothing \n",
        "<br>3 * 2 * 2 = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model of T5 Small\n",
            "\n",
            "_name_or_path : tmp_ckpt/model_cghqta/epoch_5\n",
            "model : t5-small\n",
            "epoch : 12\n",
            "batch : 64\n",
            "lr : 0.0005  [Grid Search Result]\n",
            "gradient_accumulation_steps : 4  [Grid Search Result]\n",
            "label_smoothing : 0.15  [Grid Search Result]\n"
          ]
        }
      ],
      "source": [
        "with open('tmp_ckpt_t5_small_0818/best_model/config.json', 'r') as f:\n",
        "    t5_small_best_model_config = json.load(f)\n",
        "\n",
        "with open('tmp_ckpt_t5_small_0818/best_model/trainer_config.json', 'r') as f:\n",
        "    t5_small_best_model_hyperparameter_search = json.load(f)\n",
        "\n",
        "hyperparameters = ['model', 'epoch', 'batch',\n",
        "                   'lr', 'gradient_accumulation_steps', 'label_smoothing']\n",
        "grid_serach_hyperparameter = ['lr', 'gradient_accumulation_steps', 'label_smoothing']\n",
        "\n",
        "print(colored(\"Best Model of T5 Small\", attrs=['bold']))\n",
        "print()\n",
        "for value, key in t5_small_best_model_config.items():\n",
        "    if '_name_or_path' in value:\n",
        "        print(\"{} : {}\".format(value, key))   \n",
        "    \n",
        "for value, key in t5_small_best_model_hyperparameter_search.items():\n",
        "    if any(hyperparameter in value for hyperparameter in hyperparameters):\n",
        "        if any(hyperparameter in value for hyperparameter in grid_serach_hyperparameter):\n",
        "            print(\"{} : {}  [Grid Search Result]\".format(value, key))\n",
        "        else:\n",
        "            print(\"{} : {}\".format(value, key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best T5 SmallModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### End2end QAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lmqg/t5-small-squad-qag\n",
        "CUDA_VISIBLE_DEVICES=\"0\"\n",
        "\n",
        "!python -m lm-question-generation.lmqg.grid_searcher_t5_small_squad_qag \\\n",
        "#    --checkpoint_dir='tmp_ckpt_t5_small_squad_qag_0821' \\\n",
        "#    --dataset_path='lmqg/qg_squadshifts' \\\n",
        "#    --dataset_name='all' \\\n",
        "#    --model='t5-small' \\\n",
        "#    --epoch=15 \\\n",
        "#    --epoch_partial=5 \\\n",
        "#    --batch=64 \\\n",
        "#    --n_max_config=5 \\\n",
        "#    --gradient_accumulation_steps=[4] \\\n",
        "#    --lr=[5e-04] \\\n",
        "#    --label_smoothing=[0, 0.15] \\\n",
        "#    --language=\"en\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multitask QAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CUDA_VISIBLE_DEVICES=\"0\"\n",
        "\n",
        "!python -m lm-question-generation.lmqg.grid_searcher_t5_small_squad_qg_ae \\\n",
        "#    --checkpoint_dir='tmp_ckpt_t5_small_squad_qg_ae_0821 \\\n",
        "#    --dataset_path='lmqg/qg_squadshifts' \\\n",
        "#    --dataset_name='all' \\\n",
        "#    --model='t5-small' \\\n",
        "#    --epoch=15 \\\n",
        "#    --epoch_partial=5 \\\n",
        "#    --batch=64 \\\n",
        "#    --n_max_config=5 \\\n",
        "#    --gradient_accumulation_steps=[4] \\\n",
        "#    --lr=[5e-04] \\\n",
        "#    --label_smoothing=[0, 0.15] \\\n",
        "#    --language=\"en\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipeline QAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CUDA_VISIBLE_DEVICES=\"0\"\n",
        "\n",
        "!python -m lm-question-generation.lmqg.grid_searcher_t5_small_squad_qg \\\n",
        "#    --checkpoint_dir='tmp_ckpt_t5_small_squad_qg_0821 \\\n",
        "#    --dataset_path='lmqg/t5-small-squad-qg' \\\n",
        "#    --dataset_name='all' \\\n",
        "#    --model='t5-small' \\\n",
        "#    --epoch=15 \\\n",
        "#    --epoch_partial=5 \\\n",
        "#    --batch=64 \\\n",
        "#    --n_max_config=5 \\\n",
        "#    --gradient_accumulation_steps=[4] \\\n",
        "#    --lr=[5e-04] \\\n",
        "#    --label_smoothing=[0, 0.15] \\\n",
        "#    --language=\"en\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[lmqg/t5-small-squad-qa](https://huggingface.co/lmqg/t5-small-squad-qa)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
