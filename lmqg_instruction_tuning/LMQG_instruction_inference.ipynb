{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teHzLo4owK3j"
      },
      "source": [
        "### Insturction Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import transformers\n",
        "import datasets \n",
        "from pprint import pprint\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline \n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from transformers import AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace\n"
          ]
        }
      ],
      "source": [
        "cd workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "2\n",
            "NVIDIA GeForce RTX 3090\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 32101. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['yes']\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
        "\n",
        "ADDITIONAL_SP_TOKENS = {'hl': '<hl>'}\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': list(ADDITIONAL_SP_TOKENS.values())})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.eval()\n",
        "        \n",
        "task_prefix = 'Given a passage and a highlighted answer, your goal is to generate a question about the answer. If you make a question, yes or no for first token \"When\" ? \"There are six types of first token possible: \\\"What\\\", \\\"How\\\", \\\"Who\\\", \\\"When\\\", \\\"Where\\\", \\\"Which\\\",  \\\"Why\\\".\"'\n",
        "# use different length sentences to test batching\n",
        "sentences = [\"The majority report of the Financial Crisis Inquiry Commission, written by the six Democratic appointees, the minority report, written by 3 of the <hl> 4 <hl> Republican appointees, studies by Federal Reserve economists, and the work of several independent scholars generally contend that government affordable housing policy was not the primary cause of the financial crisis. Although they concede that governmental policies had some role in causing the crisis, they contend that GSE loans performed better than loans securitized by private investment banks, and performed better than some loans originated by institutions that held loans in their own portfolios. Paul Krugman has even claimed that the GSE never purchased subprime loans ‚Äì a claim that is widely disputed.\"]\n",
        "inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "output_sequences = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    do_sample=False,  # disable sampling to test if batching affects output\n",
        ")\n",
        "\n",
        "print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminseok0809\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/wandb/run-20231026_081754-5wvg5dqt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpost_flan_t5_large_squad_qg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/minseok0809/lmqg_qg_squad\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/minseok0809/lmqg_qg_squad/runs/5wvg5dqt\u001b[0m\n",
            "\n",
            "\n",
            "Evaluation Time:  0:00:05\n",
            "Valid Bleu 1:  0.592     Valid Bleu 2: 0.4391\n",
            "Valid Bleu 3:  0.3469     Valid Bleu 4:  0.2808\n",
            "Test Bleu 1:  0.6022     Test Bleu 2:  0.4432\n",
            "Test Bleu 3:  0.346     Test Bleu 4:  0.2763\n",
            "\n",
            "\n",
            "/opt/conda/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "tmp_ckpt_flan_t5_large_squad_qg/model_airbux/epoch_18 does not appear to have a file named config.json. Checkout 'https://huggingface.co/tmp_ckpt_flan_t5_large_squad_qg/model_airbux/epoch_18/None' for available files.\n",
            "\n",
            "\n",
            "Evaluation Time:  0:00:04\n",
            "Valid Bleu 1:  0.592     Valid Bleu 2: 0.4391\n",
            "Valid Bleu 3:  0.3469     Valid Bleu 4:  0.2808\n",
            "Test Bleu 1:  0.6022     Test Bleu 2:  0.4432\n",
            "Test Bleu 3:  0.346     Test Bleu 4:  0.2763\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m lm-question-generation.lmqg_inference.post_flan_t5_large_squad_qg_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# automatic evaluationÏóêÏÑú for idx, pair in enumerate(_pairs)Ïùò if idx<=10 Î∞îÍæ∏Í∏∞\n",
        "\"\"\"\n",
        "if os.path.isfile(prediction_df_path) == True:\n",
        "    prediction_df_path = 'prediction/prediction_validation.xlsx'\n",
        "    \n",
        "elif os.path.isfile(prediction_df_path) == False: \n",
        "    prediction_df_path = 'prediction/prediction_test.xlsx' \"\"\" \n",
        "# ÌååÏùºÎ™Ö Ïù¥Î¶Ñ Î∞îÍæ∏Í∏∞ \n",
        "\n",
        "# langugage_model.pyÏùò for idx, encode in enumerate(loader)Ïùò if idx<= 10 Î∞îÍæ∏Í∏∞\n",
        "\"\"\"\n",
        "if os.path.isfile(prediction_df_path) == True:\n",
        "    prediction_df_path = 'prediction/prediction_validation.xlsx'\n",
        "    \n",
        "elif os.path.isfile(prediction_df_path) == False: \n",
        "    prediction_df_path = 'prediction/prediction_test.xlsx' \"\"\" \n",
        "# ÌååÏùºÎ™Ö Ïù¥Î¶Ñ Î∞îÍæ∏Í∏∞ \n",
        "\n",
        "# langugage_model.pyÏùò def text_to_encodeÏùò Ï£ºÏÑùÏ≤òÎ¶¨ÌïòÍ∏∞ (Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ ÌïòÍ±∞ÎÇò cache Îç∞Ïù¥ÌÑ∞ ÌååÏùº Î≥ÄÍ≤ΩÌï† Îïå)\n",
        "\"\"\"\n",
        "if cache_path is not None and os.path.exists(cache_path):\n",
        "    logging.info(f'loading preprocessed feature from {cache_path}')\n",
        "    return pickle_load(cache_path)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_sample = pd.read_csv(\"data_sample/test_sample.csv\")\n",
        "test_prediction = pd.read_csv(\"prediction/prediction_test.csv\")\n",
        "test_prediction['Label'] = test_sample['question'].values.tolist()\n",
        "test_prediction.to_csv(\"prediction/test_comparison.csv\", index=False)\n",
        "test_prediction.to_excel(\"prediction/test_comparison.xlsx\", index=False)\n",
        "dev_sample = pd.read_csv(\"data_sample/dev_sample.csv\")\n",
        "dev_prediction = pd.read_csv(\"prediction/prediction_validation.csv\")\n",
        "dev_prediction.to_csv(\"data/dev.csv\")\n",
        "dev_prediction['Label'] = dev_sample['question'].values.tolist()\n",
        "dev_prediction.to_csv(\"prediction/dev_comparison.csv\", index=False)\n",
        "dev_prediction.to_excel(\"prediction/dev_comparison.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminseok0809\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/wandb/run-20231031_030656-dhcp8wuf\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minstruction_flan_t5_large_squad_qg_q\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/minseok0809/lmqg_qg_squad\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/minseok0809/lmqg_qg_squad/runs/dhcp8wuf\u001b[0m\n",
            "/opt/conda/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:1006: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:2351: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32101. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "/opt/conda/lib/python3.8/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=False' instead.\n",
            "  warnings.warn(\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11877/11877 [00:08<00:00, 1332.64it/s]\n"
          ]
        }
      ],
      "source": [
        "!python -m lm-question-generation.lmqg_collate_fn_inference.instruction_flan_t5_large_squad_qg_evaluation_q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token Prediction by Mixed Subtask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_insturction = pd.read_csv(\"workspace/data_wh_plus/test_instruction.csv\")\n",
        "test_insturction['input'] = test_insturction['instruction'] + \" : \" + test_insturction['paragraph_answer']\n",
        "test_insturction_next_token = test_insturction[test_insturction['input'].str.contains(\"Given a paragraph and an answer, what is next\")]\n",
        "test_insturction_classification = test_insturction[test_insturction['input'].str.contains(\"Given a passage and a highlighted answer, your goal is to generate a question about the answer.\")]\n",
        "test_insturction_next_token = test_insturction_next_token.reset_index()\n",
        "del test_insturction_next_token['index']\n",
        "test_insturction_classification = test_insturction_classification.reset_index()\n",
        "del test_insturction_classification['index']\n",
        "test_insturction_next_token.to_csv(\"workspace/data/test_next_token.csv\")\n",
        "test_insturction_classification.to_csv(\"workspace/data/test_classification.csv\")\n",
        "\n",
        "train_insturction = pd.read_csv(\"workspace/data_wh_plus/train_instruction.csv\")\n",
        "train_insturction['input'] = train_insturction['instruction'] + \" : \" + train_insturction['paragraph_answer']\n",
        "train_insturction_next_token = train_insturction[train_insturction['input'].str.contains(\"Given a paragraph and an answer, what is next\")]\n",
        "train_insturction_classification = train_insturction[train_insturction['input'].str.contains(\"Given a passage and a highlighted answer, your goal is to generate a question about the answer.\")]\n",
        "train_insturction_next_token = train_insturction_next_token.reset_index()\n",
        "del train_insturction_next_token['index']\n",
        "train_insturction_classification = train_insturction_classification.reset_index()\n",
        "del train_insturction_classification['index']\n",
        "train_insturction_next_token.to_csv(\"workspace/data/train_next_token.csv\")\n",
        "train_insturction_classification.to_csv(\"workspace/data/train_classification.csv\")\n",
        "\n",
        "dev_insturction = pd.read_csv(\"workspace/data_wh_plus/dev_instruction.csv\")\n",
        "dev_insturction['input'] = dev_insturction['instruction'] + \" : \" + dev_insturction['paragraph_answer']\n",
        "dev_insturction_next_token = dev_insturction[dev_insturction['input'].str.contains(\"Given a paragraph and an answer, what is next\")]\n",
        "dev_insturction_classification = dev_insturction[dev_insturction['input'].str.contains(\"Given a passage and a highlighted answer, your goal is to generate a question about the answer.\")]\n",
        "dev_insturction_next_token = dev_insturction_next_token.reset_index()\n",
        "del dev_insturction_next_token['index']\n",
        "dev_insturction_classification = dev_insturction_classification.reset_index()\n",
        "del dev_insturction_classification['index']\n",
        "dev_insturction_next_token.to_csv(\"workspace/data/dev_next_token.csv\")\n",
        "dev_insturction_classification.to_csv(\"workspace/data/dev_classification.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.5037424537281828}\n",
            "\n",
            "{'bleu 2': 0.4771999325691208}\n",
            "\n",
            "{'bleu 3': 0.4506490222521915}\n",
            "\n",
            "{'bleu 4': 0.3849460552933243}\n",
            "\n",
            "\n",
            "Text Size: 9018\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 \n",
            "\n",
            "Accuracy: 0.1317365269461078\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/small_model_wh_plus/epoch_3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data/test_classification.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "data_files = {\"train\": \"train_classification.csv\", \"validation\": \"dev_classification.csv\", \"test\": \"test_classification.csv\"}\n",
        "dataset = load_dataset(\"workspace/data/\", data_files=data_files)\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_plus_small_classifcation_epoch_3.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_plus_small_classifcation_epoch_3.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_plus_small_classifcation_epoch_3.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def compute_metrics(reference_texts, generated_texts, blue_num):\n",
        "    # print(\"The number of sentece:\", len(reference_texts))\n",
        "    bleu_scores = []\n",
        "    for idx, (reference_text, generated_text) in enumerate(zip(reference_texts, generated_texts)):\n",
        "        #if idx % 1000 == 0:\n",
        "        #    print(idx, end=\" \")\n",
        "        reference = reference_text.split()\n",
        "        candidate = generated_text.split()\n",
        "        if blue_num == 1:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0))\n",
        "        elif blue_num == 2:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 1, 0, 0))\n",
        "        elif blue_num == 3:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 0, 1, 0))\n",
        "        elif blue_num == 4:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 0, 0, 1))\n",
        "        bleu_scores.append(bleu_score)\n",
        "    \n",
        "    return {\n",
        "            'bleu' + ' ' + str(blue_num) : sum(bleu_scores) / len(bleu_scores)\n",
        "        }\n",
        "    \n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "\n",
        "print(\"\\n\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/small_model_wh_plus/epoch_3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data/test_next_token.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "data_files = {\"train\": \"train_next_token.csv\", \"validation\": \"dev_next_token.csv\", \"test\": \"test_next_token.csv\"}\n",
        "dataset = load_dataset(\"workspace/data/\", data_files=data_files)\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_plus_small_next_token_epoch_3.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_plus_small_next_token_epoch_3.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_plus_small_next_token_epoch_3.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "\n",
        "def evaluate(y_test, x_test):\n",
        "    sum = 0\n",
        "    for i, j in zip(y_test, x_test):\n",
        "        if i == j:\n",
        "            sum += 1\n",
        "    accuracy = sum / len(y_test)\n",
        "    return accuracy \n",
        "\n",
        "def get_counts(seq): \n",
        "    counts = {}\n",
        "    for x in seq:\n",
        "        if x in counts:\n",
        "            counts[x] += 1\n",
        "        else:\n",
        "            counts[x] = 1\n",
        "    return counts\n",
        "\n",
        "accuracy = evaluate(prediction, label)\n",
        "llmqg_counts = get_counts(prediction)\n",
        "inference_counts = get_counts(label)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "prediction_df = pd.DataFrame(list(llmqg_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "inference_df = pd.DataFrame(list(inference_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "prediction_df.to_excel(\"workspace/inference/predcition_counts_wh_plus_small_next_token_epoch_3.xlsx\", index=False)\n",
        "inference_df.to_excel(\"workspace/inference/inference_counts_wh_plus_small_next_token_epoch_3.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0059206485748291016,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading data files",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d3e167164a2448eaeb7caaeff004de2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.002970457077026367,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Extracting data files",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "defb79326cea46359ae2fe8f3c70b6d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.00305938720703125,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Generating train split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a09ce40a59047098ff02e67b92a67c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0034003257751464844,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Generating validation split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46f40000688b40b8b37e54b33bbd01b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.003361225128173828,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Generating test split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dec824022204cffb19516e02ea3b692",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.5770079266476259}\n",
            "\n",
            "{'bleu 2': 0.5632305012362352}\n",
            "\n",
            "{'bleu 3': 0.5434086311530681}\n",
            "\n",
            "{'bleu 4': 0.5149190829399866}\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.005247592926025391,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading data files",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcc669f03dcb4cdabe83299cf5e43106",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.003080606460571289,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Extracting data files",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1a48b6e3c7b41ec98c0a6a7b0230a40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.00398707389831543,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Generating train split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1cd490a19d04aa0870a219b21ce2f09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0034105777740478516,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Generating validation split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d0efea496fd421da1e62d2c2c56417a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0046138763427734375,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Generating test split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0078d23ca62e4999b3dae53146295955",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 9018\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 \n",
            "\n",
            "Accuracy: 0.2162341982701264\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/small_model_wh_plus/epoch_6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data/test_classification.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "data_files = {\"train\": \"train_classification.csv\", \"validation\": \"dev_classification.csv\", \"test\": \"test_classification.csv\"}\n",
        "dataset = load_dataset(\"workspace/data/\", data_files=data_files)\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_plus_small_classifcation_epoch_6.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_plus_small_classifcation_epoch_6.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_plus_small_classifcation_epoch_6.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def compute_metrics(reference_texts, generated_texts, blue_num):\n",
        "    # print(\"The number of sentece:\", len(reference_texts))\n",
        "    bleu_scores = []\n",
        "    for idx, (reference_text, generated_text) in enumerate(zip(reference_texts, generated_texts)):\n",
        "        #if idx % 1000 == 0:\n",
        "        #    print(idx, end=\" \")\n",
        "        reference = reference_text.split()\n",
        "        candidate = generated_text.split()\n",
        "        if blue_num == 1:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0))\n",
        "        elif blue_num == 2:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 1, 0, 0))\n",
        "        elif blue_num == 3:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 0, 1, 0))\n",
        "        elif blue_num == 4:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 0, 0, 1))\n",
        "        bleu_scores.append(bleu_score)\n",
        "    \n",
        "    return {\n",
        "            'bleu' + ' ' + str(blue_num) : sum(bleu_scores) / len(bleu_scores)\n",
        "        }\n",
        "    \n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "\n",
        "print(\"\\n\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/small_model_wh_plus/epoch_6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data/test_next_token.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "data_files = {\"train\": \"train_next_token.csv\", \"validation\": \"dev_next_token.csv\", \"test\": \"test_next_token.csv\"}\n",
        "dataset = load_dataset(\"workspace/data/\", data_files=data_files)\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_plus_small_next_token_epoch_6.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_plus_small_next_token_epoch_6.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_plus_small_next_token_epoch_6.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "\n",
        "def evaluate(y_test, x_test):\n",
        "    sum = 0\n",
        "    for i, j in zip(y_test, x_test):\n",
        "        if i == j:\n",
        "            sum += 1\n",
        "    accuracy = sum / len(y_test)\n",
        "    return accuracy \n",
        "\n",
        "def get_counts(seq): \n",
        "    counts = {}\n",
        "    for x in seq:\n",
        "        if x in counts:\n",
        "            counts[x] += 1\n",
        "        else:\n",
        "            counts[x] = 1\n",
        "    return counts\n",
        "\n",
        "accuracy = evaluate(prediction, label)\n",
        "llmqg_counts = get_counts(prediction)\n",
        "inference_counts = get_counts(label)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "prediction_df = pd.DataFrame(list(llmqg_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "inference_df = pd.DataFrame(list(inference_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "prediction_df.to_excel(\"workspace/inference/predcition_counts_wh_plus_small_next_token_epoch_6.xlsx\", index=False)\n",
        "inference_df.to_excel(\"workspace/inference/inference_counts_wh_plus_small_next_token_epoch_6.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/small_model_wh_plus/epoch_6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data/test_classification.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "data_files = {\"train\": \"train_classification.csv\", \"validation\": \"dev_classification.csv\", \"test\": \"test_classification.csv\"}\n",
        "dataset = load_dataset(\"workspace/data/\", data_files=data_files)\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_plus_small_classifcation_epoch_6.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_plus_small_classifcation_epoch_6.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_plus_small_classifcation_epoch_6.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def compute_metrics(reference_texts, generated_texts, blue_num):\n",
        "    # print(\"The number of sentece:\", len(reference_texts))\n",
        "    bleu_scores = []\n",
        "    for idx, (reference_text, generated_text) in enumerate(zip(reference_texts, generated_texts)):\n",
        "        #if idx % 1000 == 0:\n",
        "        #    print(idx, end=\" \")\n",
        "        reference = reference_text.split()\n",
        "        candidate = generated_text.split()\n",
        "        if blue_num == 1:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0))\n",
        "        elif blue_num == 2:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 1, 0, 0))\n",
        "        elif blue_num == 3:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 0, 1, 0))\n",
        "        elif blue_num == 4:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 0, 0, 1))\n",
        "        bleu_scores.append(bleu_score)\n",
        "    \n",
        "    return {\n",
        "            'bleu' + ' ' + str(blue_num) : sum(bleu_scores) / len(bleu_scores)\n",
        "        }\n",
        "    \n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "\n",
        "print(\"\\n\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/small_model_wh_plus/epoch_6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data/test_next_token.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "data_files = {\"train\": \"train_next_token.csv\", \"validation\": \"dev_next_token.csv\", \"test\": \"test_next_token.csv\"}\n",
        "dataset = load_dataset(\"workspace/data/\", data_files=data_files)\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_plus_small_next_token_epoch_6.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_plus_small_next_token_epoch_6.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_plus_small_next_token_epoch_6.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "\n",
        "def evaluate(y_test, x_test):\n",
        "    sum = 0\n",
        "    for i, j in zip(y_test, x_test):\n",
        "        if i == j:\n",
        "            sum += 1\n",
        "    accuracy = sum / len(y_test)\n",
        "    return accuracy \n",
        "\n",
        "def get_counts(seq): \n",
        "    counts = {}\n",
        "    for x in seq:\n",
        "        if x in counts:\n",
        "            counts[x] += 1\n",
        "        else:\n",
        "            counts[x] = 1\n",
        "    return counts\n",
        "\n",
        "accuracy = evaluate(prediction, label)\n",
        "llmqg_counts = get_counts(prediction)\n",
        "inference_counts = get_counts(label)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "prediction_df = pd.DataFrame(list(llmqg_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "inference_df = pd.DataFrame(list(inference_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "prediction_df.to_excel(\"workspace/inference/predcition_counts_wh_plus_small_next_token_epoch_6.xlsx\", index=False)\n",
        "inference_df.to_excel(\"workspace/inference/inference_counts_wh_plus_small_next_token_epoch_6.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token Prediction by Random Long WH Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def compute_metrics(reference_texts, generated_texts, blue_num):\n",
        "    # print(\"The number of sentece:\", len(reference_texts))\n",
        "    bleu_scores = []\n",
        "    for idx, (reference_text, generated_text) in enumerate(zip(reference_texts, generated_texts)):\n",
        "        #if idx % 1000 == 0:\n",
        "        #    print(idx, end=\" \")\n",
        "        reference = reference_text.split()\n",
        "        candidate = generated_text.split()\n",
        "        if blue_num == 1:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0))\n",
        "        elif blue_num == 2:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 1, 0, 0))\n",
        "        elif blue_num == 3:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 0, 1, 0))\n",
        "        elif blue_num == 4:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 0, 0, 1))\n",
        "        bleu_scores.append(bleu_score)\n",
        "    \n",
        "    return {\n",
        "            'bleu' + ' ' + str(blue_num) : sum(bleu_scores) / len(bleu_scores)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lmqg-eval -m \"lmqg/t5-large-squad-qg\" -e \"./eval_metrics\" -d \"lmqg/qg_squad\" -l \"en\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "test_insturction['input'] = test_insturction['instruction'] + \" : \" + test_insturction['paragraph_answer']\n",
        "test_insturction.to_csv(\"workspace/data/test.csv\")\n",
        "train_insturction = pd.read_csv(\"workspace/data_wh_zero/train_instruction.csv\")\n",
        "train_insturction['input'] = train_insturction['instruction'] + \" : \" + train_insturction['paragraph_answer']\n",
        "train_insturction.to_csv(\"workspace/data/train.csv\")\n",
        "dev_insturction = pd.read_csv(\"workspace/data_wh_zero/dev_instruction.csv\")\n",
        "dev_insturction['input'] = dev_insturction['instruction'] + \" : \" + dev_insturction['paragraph_answer']\n",
        "dev_insturction.to_csv(\"workspace/data/dev.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.008671283721923828,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading data files",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e416ec022efd4ddfae345fd3002cce5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.002844572067260742,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Extracting data files",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "642165ee60cd4f29be737c55ad6bc22d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0031223297119140625,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Generating train split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a5f4db723ab47a580633418a1fdcdd8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0035436153411865234,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Generating validation split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89ff71728080465b984eb4856c069b95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0033521652221679688,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Generating test split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9e863a4c10d41ab97e1c3100458e8a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data_files = {\"train\": \"train.csv\", \"validation\": \"dev.csv\", \"test\": \"test.csv\"}\n",
        "dataset = load_dataset(\"workspace/data/\", data_files=data_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.5392525393278271}\n",
            "\n",
            "{'bleu 2': 0.5290138581672758}\n",
            "\n",
            "{'bleu 3': 0.5111682400539447}\n",
            "\n",
            "{'bleu 4': 0.4939312204989885}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/tmp_instruction_flan_t5_large_squad_qg_wh_zero/model_mtbhfb/epoch_1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_large_epoch_1.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_large_epoch_1.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_large_epoch_1.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.8892974642190106}\n",
            "\n",
            "{'bleu 2': 0.883723870532704}\n",
            "\n",
            "{'bleu 3': 0.8605866486850978}\n",
            "\n",
            "{'bleu 4': 0.8379130141604855}\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/tmp_instruction_flan_t5_large_squad_qg_wh_zero/model_mtbhfb/epoch_3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_large_epoch_1.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_large_epoch_1.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_large_epoch_1.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.9307220006778704}\n",
            "\n",
            "{'bleu 2': 0.9170030341807663}\n",
            "\n",
            "{'bleu 3': 0.888668931319505}\n",
            "\n",
            "{'bleu 4': 0.8627595078205555}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/tmp_instruction_flan_t5_large_squad_qg_wh_zero/model_mtbhfb/epoch_6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_large_epoch_1.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_large_epoch_1.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_large_epoch_1.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.5392525393278271}\n",
            "\n",
            "{'bleu 2': 0.5290138581672758}\n",
            "\n",
            "{'bleu 3': 0.5111682400539447}\n",
            "\n",
            "{'bleu 4': 0.4939312204989885}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/tmp_instruction_flan_t5_large_squad_qg_wh_zero/model_mtbhfb/epoch_1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_large_epoch_1.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_large_epoch_1.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_large_epoch_1.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.8697595126176412}\n",
            "\n",
            "{'bleu 2': 0.8636912789390874}\n",
            "\n",
            "{'bleu 3': 0.8414531355360755}\n",
            "\n",
            "{'bleu 4': 0.8190323668240054}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/tmp_instruction_flan_t5_large_squad_qg_wh_zero/model_mtbhfb/epoch_2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_large_epoch_2.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_large_epoch_2.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_large_epoch_2.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.8892974642190106}\n",
            "\n",
            "{'bleu 2': 0.883723870532704}\n",
            "\n",
            "{'bleu 3': 0.8605866486850978}\n",
            "\n",
            "{'bleu 4': 0.8379130141604855}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/tmp_instruction_flan_t5_large_squad_qg_wh_zero/model_mtbhfb/epoch_3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_large_epoch_3.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_large_epoch_3.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_large_epoch_3.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.8982885575152644}\n",
            "\n",
            "{'bleu 2': 0.8927848954821311}\n",
            "\n",
            "{'bleu 3': 0.8692683749157114}\n",
            "\n",
            "{'bleu 4': 0.8460047201618341}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/tmp_instruction_flan_t5_large_squad_qg_wh_zero/model_mtbhfb/epoch_4\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_large_epoch_4.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_large_epoch_4.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_large_epoch_4.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.9139666795419563}\n",
            "\n",
            "{'bleu 2': 0.9057533939358806}\n",
            "\n",
            "{'bleu 3': 0.8799730276466622}\n",
            "\n",
            "{'bleu 4': 0.855192178017532}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/tmp_instruction_flan_t5_large_squad_qg_wh_zero/model_mtbhfb/epoch_5\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_large_epoch_5.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_large_epoch_5.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_large_epoch_5.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.9307220006778704}\n",
            "\n",
            "{'bleu 2': 0.9170030341807663}\n",
            "\n",
            "{'bleu 3': 0.888668931319505}\n",
            "\n",
            "{'bleu 4': 0.8627595078205555}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/tmp_instruction_flan_t5_large_squad_qg_wh_zero/model_mtbhfb/epoch_6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_large_epoch_6.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_large_epoch_6.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_large_epoch_6.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Small Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'bleu 1': 0.5053509612579646}\n",
            "\n",
            "{'bleu 2': 0.47682063385030765}\n",
            "\n",
            "{'bleu 3': 0.44951112609575183}\n",
            "\n",
            "{'bleu 4': 0.3796358732299393}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/model_wh_zero/epoch_3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_epoch_3.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_epoch_3.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_epoch_3.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 {'bleu 1': 0.5279754274301385}\n",
            "\n",
            "{'bleu 2': 0.5030765340526012}\n",
            "\n",
            "{'bleu 3': 0.47850640593391774}\n",
            "\n",
            "{'bleu 4': 0.418745785569791}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/model_wh_zero/epoch_7\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_epoch_7.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_epoch_7.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_epoch_7.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 {'bleu 1': 0.7904469335827348}\n",
            "\n",
            "{'bleu 2': 0.7795666554941918}\n",
            "\n",
            "{'bleu 3': 0.7567244437612164}\n",
            "\n",
            "{'bleu 4': 0.724020465339099}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/model_wh_zero/epoch_10\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_epoch_10.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_epoch_10.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_epoch_10.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.8477668401617663}\n",
            "\n",
            "{'bleu 2': 0.8408099160108934}\n",
            "\n",
            "{'bleu 3': 0.8187795010114632}\n",
            "\n",
            "{'bleu 4': 0.7934929197572488}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/model_wh_zero/epoch_13\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_epoch_13.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_epoch_13.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_epoch_13.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11864\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "{'bleu 1': 0.8591525836414106}\n",
            "\n",
            "{'bleu 2': 0.8530569323070248}\n",
            "\n",
            "{'bleu 3': 0.8318548199315497}\n",
            "\n",
            "{'bleu 4': 0.8091706001348618}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/model_wh_zero/epoch_20\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh_zero/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_zero_epoch_20.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_zero_epoch_20.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_zero_epoch_20.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(\"\\n\")\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token Prediction by Long WH Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(y_test, x_test):\n",
        "    sum = 0\n",
        "    for i, j in zip(y_test, x_test):\n",
        "        if i == j:\n",
        "            sum += 1\n",
        "    accuracy = sum / len(y_test)\n",
        "    return accuracy \n",
        "\n",
        "def get_counts(seq): \n",
        "    counts = {}\n",
        "    for x in seq:\n",
        "        if x in counts:\n",
        "            counts[x] += 1\n",
        "        else:\n",
        "            counts[x] = 1\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_insturction = pd.read_csv(\"workspace/data_wh/test_instruction.csv\")\n",
        "test_insturction['input'] = test_insturction['instruction'] + \" : \" + test_insturction['paragraph_answer']\n",
        "test_insturction.to_csv(\"workspace/data/test.csv\")\n",
        "train_insturction = pd.read_csv(\"workspace/data_wh/train_instruction.csv\")\n",
        "train_insturction['input'] = train_insturction['instruction'] + \" : \" + train_insturction['paragraph_answer']\n",
        "train_insturction.to_csv(\"workspace/data/train.csv\")\n",
        "dev_insturction = pd.read_csv(\"workspace/data_wh/dev_instruction.csv\")\n",
        "dev_insturction['input'] = dev_insturction['instruction'] + \" : \" + dev_insturction['paragraph_answer']\n",
        "dev_insturction.to_csv(\"workspace/data/dev.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.006300210952758789,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading data files",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73478e1895f84918917817a7a8bf7702",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.002873659133911133,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Extracting data files",
              "rate": null,
              "total": 3,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "057bfe1fc30e44b9aa6583b75e4cb1d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.003498077392578125,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Generating train split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e80055ba47d348f8ae5aa04cbaa2f42f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.003401517868041992,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Generating validation split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e60c2849681b4ee9a79bf72f2b5aba17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0035049915313720703,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Generating test split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4367a06281a4074986eb0e31901990a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data_files = {\"train\": \"train.csv\", \"validation\": \"dev.csv\", \"test\": \"test.csv\"}\n",
        "dataset = load_dataset(\"workspace/data/\", data_files=data_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/large_model_wh/epoch_1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_long_large_epoch1.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_long_large_epoch1.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_long_large_epoch1.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "\n",
        "accuracy = evaluate(prediction, label)\n",
        "llmqg_counts = get_counts(prediction)\n",
        "inference_counts = get_counts(label)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "prediction_df = pd.DataFrame(list(llmqg_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "inference_df = pd.DataFrame(list(inference_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "prediction_df.to_excel(\"workspace/inference/predcition_counts_wh_long_large_epoch1.xlsx\", index=False)\n",
        "inference_df.to_excel(\"workspace/inference/inference_counts_wh_long_large_epoch1.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11289\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 0.5377801399592523\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/model_wh/epoch_3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_long_epoch3.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_long_epoch3.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_long_epoch3.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "\n",
        "accuracy = evaluate(prediction, label)\n",
        "llmqg_counts = get_counts(prediction)\n",
        "inference_counts = get_counts(label)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "prediction_df = pd.DataFrame(list(llmqg_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "inference_df = pd.DataFrame(list(inference_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "prediction_df.to_excel(\"workspace/inference/predcition_counts_wh_long_epoch3.xlsx\", index=False)\n",
        "inference_df.to_excel(\"workspace/inference/inference_counts_wh_long_epoch3.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11289\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "Accuracy: 0.5381344671804411\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/model_wh/epoch_7\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_long_epoch7.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_long_epoch3.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_long_epoch7.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "\n",
        "accuracy = evaluate(prediction, label)\n",
        "llmqg_counts = get_counts(prediction)\n",
        "inference_counts = get_counts(label)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "prediction_df = pd.DataFrame(list(llmqg_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "inference_df = pd.DataFrame(list(inference_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "prediction_df.to_excel(\"workspace/inference/predcition_counts_wh_long_epoch7.xlsx\", index=False)\n",
        "inference_df.to_excel(\"workspace/inference/inference_counts_wh_long_epoch7.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 11289\n",
            "0 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 \n",
            "\n",
            "Accuracy: 0.5381344671804411\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/model_wh/epoch_10\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_wh/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=32, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 1000 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "# result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/comparison_wh_long_epoch10.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/comparison_wh_long_epoch10.xlsx\", index=False)\n",
        "\n",
        "inference_result = pd.read_csv(\"workspace/inference/comparison_wh_long_epoch10.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "\n",
        "accuracy = evaluate(prediction, label)\n",
        "llmqg_counts = get_counts(prediction)\n",
        "inference_counts = get_counts(label)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "prediction_df = pd.DataFrame(list(llmqg_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "inference_df = pd.DataFrame(list(inference_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "prediction_df.to_excel(\"workspace/inference/predcition_counts_wh_long_epoch_10.xlsx\", index=False)\n",
        "inference_df.to_excel(\"workspace/inference/inference_counts_wh_long_epoch_10.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### WH Token Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(y_test, x_test):\n",
        "    sum = 0\n",
        "    for i, j in zip(y_test, x_test):\n",
        "        if i == j:\n",
        "            sum += 1\n",
        "    accuracy = sum / len(y_test)\n",
        "    return accuracy \n",
        "\n",
        "def get_counts(seq): \n",
        "    counts = {}\n",
        "    for x in seq:\n",
        "        if x in counts:\n",
        "            counts[x] += 1\n",
        "        else:\n",
        "            counts[x] = 1\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_insturction = pd.read_csv(\"workspace/data_first_token/test_instruction.csv\")\n",
        "test_insturction['input'] = test_insturction['instruction'] + \" : \" + test_insturction['paragraph_answer']\n",
        "test_insturction.to_csv(\"workspace/data/test.csv\")\n",
        "train_insturction = pd.read_csv(\"workspace/data_first_token/train_instruction.csv\")\n",
        "train_insturction['input'] = train_insturction['instruction'] + \" : \" + train_insturction['paragraph_answer']\n",
        "train_insturction.to_csv(\"workspace/data/train.csv\")\n",
        "dev_insturction = pd.read_csv(\"workspace/data_first_token/dev_instruction.csv\")\n",
        "dev_insturction['input'] = dev_insturction['instruction'] + \" : \" + dev_insturction['paragraph_answer']\n",
        "dev_insturction.to_csv(\"workspace/data/dev.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_files = {\"train\": \"train.csv\", \"validation\": \"dev.csv\", \"test\": \"test.csv\"}\n",
        "dataset = load_dataset(\"workspace/data/\", data_files=data_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 8142\n",
            "0 100 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 3400 3500 3600 3700 3800 3900 4000 4100 4200 4300 4400 4500 4600 4700 4800 4900 5000 5100 5200 5300 5400 5500 5600 5700 5800 5900 6000 6100 6200 6300 6400 6500 6600 6700 6800 6900 7000 7100 7200 7300 7400 7500 7600 7700 7800 7900 8000 8100 "
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/token_3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_first_token/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=8, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 100 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/result3.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/result3.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.43711618766887744\n"
          ]
        }
      ],
      "source": [
        "inference_result = pd.read_csv(\"workspace/inference/result3.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "\n",
        "accuracy = evaluate(prediction, label)\n",
        "llmqg_counts = get_counts(prediction)\n",
        "inference_counts = get_counts(label)\n",
        "print(accuracy)\n",
        "prediction_df = pd.DataFrame(list(llmqg_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "inference_df = pd.DataFrame(list(inference_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "prediction_df.to_excel(\"workspace/inference/lmqg_counts3.xlsx\", index=False)\n",
        "inference_df.to_excel(\"workspace/inference/inference_counts3.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next Token Predicition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(y_test, x_test):\n",
        "    sum = 0\n",
        "    for i, j in zip(y_test, x_test):\n",
        "        if i == j:\n",
        "            sum += 1\n",
        "    accuracy = sum / len(y_test)\n",
        "    return accuracy \n",
        "\n",
        "def get_counts(seq): \n",
        "    counts = {}\n",
        "    for x in seq:\n",
        "        if x in counts:\n",
        "            counts[x] += 1\n",
        "        else:\n",
        "            counts[x] = 1\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_insturction = pd.read_csv(\"workspace/data_first_and_second/test_instruction.csv\")\n",
        "test_insturction['input'] = test_insturction['instruction'] + \" : \" + test_insturction['paragraph_answer']\n",
        "test_insturction.to_csv(\"workspace/data/test.csv\", index=False)\n",
        "train_insturction = pd.read_csv(\"workspace/data_first_and_second/train_instruction.csv\")\n",
        "train_insturction['input'] = train_insturction['instruction'] + \" : \" + train_insturction['paragraph_answer']\n",
        "train_insturction.to_csv(\"workspace/data/train.csv\", index=False)\n",
        "dev_insturction = pd.read_csv(\"workspace/data_first_and_second/dev_instruction.csv\")\n",
        "dev_insturction['input'] = dev_insturction['instruction'] + \" : \" + dev_insturction['paragraph_answer']\n",
        "dev_insturction.to_csv(\"workspace/data/dev.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_files = {\"train\": \"train.csv\", \"validation\": \"dev.csv\", \"test\": \"test.csv\"}\n",
        "dataset = load_dataset(\"workspace/data/\", data_files=data_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Size: 9018\n",
            "0 100 200 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 3400 3500 3600 3700 3800 3900 4000 4100 4200 4300 4400 4500 4600 4700 4800 4900 5000 5100 5200 5300 5400 5500 5600 5700 5800 5900 6000 6100 6200 6300 6400 6500 6600 6700 6800 6900 7000 7100 7200 7300 7400 7500 7600 7700 7800 7900 8000 8100 8200 8300 8400 8500 8600 8700 8800 8900 9000 "
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/tmp_instruction_flan_t5_large_squad_qg_first_and_second/model_fznckd/epoch_3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_first_and_second/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['answer'].values.tolist()\n",
        "predictions = []\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=20, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 100 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Answer':dataset['test']['answer']})\n",
        "result_df['Comparsion'] = result_df['Prediction'] == result_df['Answer']\n",
        "result_df.to_csv(\"workspace/inference/result2.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/result2.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.49312486138833445\n"
          ]
        }
      ],
      "source": [
        "inference_result = pd.read_csv(\"workspace/inference/result2.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['Answer']\n",
        "\n",
        "accuracy = evaluate(prediction, label)\n",
        "llmqg_counts = get_counts(prediction)\n",
        "inference_counts = get_counts(label)\n",
        "print(accuracy)\n",
        "prediction_df = pd.DataFrame(list(llmqg_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "inference_df = pd.DataFrame(list(inference_counts.items()), columns=[\"Value\", \"Count\"]).sort_values('Count', ascending=False)\n",
        "prediction_df.to_excel(\"workspace/inference/lmqg_counts.xlsx\", index=False)\n",
        "inference_df.to_excel(\"workspace/inference/inference_counts.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sentence Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def compute_metrics(reference_texts, generated_texts, blue_num):\n",
        "    # print(\"The number of sentece:\", len(reference_texts))\n",
        "    bleu_scores = []\n",
        "    for idx, (reference_text, generated_text) in enumerate(zip(reference_texts, generated_texts)):\n",
        "        #if idx % 1000 == 0:\n",
        "        #    print(idx, end=\" \")\n",
        "        reference = reference_text.split()\n",
        "        candidate = generated_text.split()\n",
        "        if blue_num == 1:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0))\n",
        "        elif blue_num == 2:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 1, 0, 0))\n",
        "        elif blue_num == 3:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 0, 1, 0))\n",
        "        elif blue_num == 4:\n",
        "            bleu_score = sentence_bleu([reference], candidate, weights=(0, 0, 0, 1))\n",
        "        bleu_scores.append(bleu_score)\n",
        "    \n",
        "    return {\n",
        "            'bleu' + ' ' + str(blue_num) : sum(bleu_scores) / len(bleu_scores)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_insturction = pd.read_csv(\"workspace/data_curious2/test_instruction.csv\")\n",
        "test_insturction['input'] = test_insturction['instruction'] + \" : \" + test_insturction['paragraph_answer']\n",
        "test_insturction.to_csv(\"workspace/data/test.csv\", index=False)\n",
        "train_insturction = pd.read_csv(\"workspace/data_curious2/train_instruction.csv\")\n",
        "train_insturction['input'] = train_insturction['instruction'] + \" : \" + train_insturction['paragraph_answer']\n",
        "train_insturction.to_csv(\"workspace/data/train.csv\", index=False)\n",
        "dev_insturction = pd.read_csv(\"workspace/data_curious2/dev_instruction.csv\")\n",
        "dev_insturction['input'] = dev_insturction['instruction'] + \" : \" + dev_insturction['paragraph_answer']\n",
        "dev_insturction.to_csv(\"workspace/data/dev.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_files = {\"train\": \"train.csv\", \"validation\": \"dev.csv\", \"test\": \"test.csv\"}\n",
        "dataset = load_dataset(\"workspace/data/\", data_files=data_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = \"workspace/tmp_instruction_flan_t5_large_squad_qg_curious2/model_qdrkjl/epoch_1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=1)\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_curious2/test_instruction.csv\")\n",
        "inference_input_sentences = test_insturction['sentence'].values.tolist()\n",
        "predictions = []\n",
        "dataset = load_dataset(\"workspace/data/\")\n",
        "\n",
        "result_j = text_generator(KeyDataset(dataset['test'], 'input'), max_length=512, batch_size=32)\n",
        "print(\"Text Size:\", len(dataset['test']))\n",
        "for idx, extracted_entities in enumerate(result_j):\n",
        "    if idx % 100 == 0:\n",
        "        print(idx, end=\" \")\n",
        "    for entity in extracted_entities:\n",
        "        predictions.append(entity['generated_text'])\n",
        "\n",
        "result_df = pd.DataFrame({'Prediction':predictions,\n",
        "                                'Sentence':dataset['test']['paragraph_answer']})\n",
        "\n",
        "result_df.to_csv(\"workspace/inference/result.csv\", index=False)\n",
        "result_df.to_excel(\"workspace/inference/result.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'bleu 1': 0.9362544497922547}\n",
            "\n",
            "{'bleu 2': 0.9335188260055463}\n",
            "\n",
            "{'bleu 3': 0.9309754167083432}\n",
            "\n",
            "{'bleu 4': 0.9283027265343414}\n"
          ]
        }
      ],
      "source": [
        "inference_result = pd.read_csv(\"workspace/inference/result.csv\")\n",
        "prediction = inference_result['Prediction']\n",
        "label = inference_result['sentence']\n",
        "bleu_num = 1\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 2\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 3\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)\n",
        "print()\n",
        "bleu_num = 4\n",
        "accuracy = compute_metrics(prediction, label, bleu_num)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "test_insturction = pd.read_csv(\"workspace/data_curious2/test_instruction.csv\")\n",
        "inference_input_instructions = test_insturction['instruction'].values.tolist()\n",
        "inference_input_sentences = test_insturction['sentence'].values.tolist()\n",
        "inference_input_paragraph_answers = test_insturction['paragraph_answer'].values.tolist()\n",
        "\n",
        "model_name = \"workspace/tmp_instruction_flan_t5_large_squad_qg_curious2/model_qdrkjl/epoch_1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "ADDITIONAL_SP_TOKENS = {'hl': '<hl>'}\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': list(ADDITIONAL_SP_TOKENS.values())})\n",
        "model.resize_token_embeddings(len(tokenizer))          \n",
        "model.eval()\n",
        "\n",
        "lmqg_model = []\n",
        "idx = 0\n",
        "num = 1000\n",
        "prefixs = inference_input_instructions\n",
        "paragraphs = inference_input_paragraph_answers\n",
        "sentences = inference_input_sentences\n",
        "\n",
        "# prefix = \"Generate setence. : \"\n",
        "# paragraph = [\"The term melting pot was first coined to describe densely populated immigrant neighborhoods on the Lower East Side. Emilly Know about it.\"]\n",
        "# sentences = [\"The majority report of the Financial Crisis Inquiry Commission, written by the six Democratic appointees, the minority report, written by 3 of the <hl> 4 <hl> Republican appointees, studies by Federal Reserve economists, and the work of several independent scholars generally contend that government affordable housing policy was not the primary cause of the financial crisis. Although they concede that governmental policies had some role in causing the crisis, they contend that GSE loans performed better than loans securitized by private investment banks, and performed better than some loans originated by institutions that held loans in their own portfolios. Paul Krugman has even claimed that the GSE never purchased subprime loans ‚Äì a claim that is widely disputed.\"]\n",
        "\n",
        "for task_prefix, sentence in zip(tqdm(prefixs), paragraphs):\n",
        "    inputs = tokenizer([task_prefix + \" : \" + sentence], return_tensors=\"pt\", padding=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        do_sample=False,  # disable sampling to test if batching affects output\n",
        "    )\n",
        "\n",
        "    lmqg_model.append(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))\n",
        "\n",
        "# pbar.close()\n",
        "result_df = pd.DataFrame({\"Prediction\": lmqg_model, \"Sentence\": sentences})\n",
        "result_df.to_excel(\"workspace/inference/sentence.xlsx\", index=False)\n",
        "result_df.to_csv(\"workspace/inference/sentence.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def compute_metrics(reference_texts, generated_texts):\n",
        "    bleu_scores = []\n",
        "    for reference_text, generated_text in zip(reference_texts, generated_texts):\n",
        "        bleu_score = sentence_bleu(reference_text, generated_text)\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "    return {\n",
        "            'bleu': sum(bleu_scores) / len(bleu_scores)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = compute_metrics(inference_input_sentences[:1000], lmqg_model)\n",
        "print(accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
